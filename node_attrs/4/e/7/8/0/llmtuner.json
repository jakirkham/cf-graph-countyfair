{
 "archived": false,
 "branch": "main",
 "conda-forge.yml": {
  "bot": {
   "automerge": true,
   "inspection": "update-grayskull"
  },
  "conda_build": {
   "error_overlinking": true
  },
  "conda_forge_output_validation": true,
  "github": {
   "branch_name": "main",
   "tooling_branch_name": "main"
  }
 },
 "feedstock_name": "llmtuner",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "home": "https://github.com/hiyouga/LLaMA-Factory",
   "license": "Apache-2.0",
   "license_file": "LICENSE",
   "summary": "Easy-to-use LLM fine-tuning framework"
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
  },
  "extra": {
   "recipe-maintainers": [
    "jan-janssen"
   ]
  },
  "package": {
   "name": "llmtuner",
   "version": "0.8.2"
  },
  "requirements": {
   "host": [
    "python >=3.8",
    "setuptools >=61.0",
    "pip"
   ],
   "run": [
    "pyyaml",
    "packaging",
    "python >=3.8",
    "pytorch >=1.13.1",
    "transformers >=4.37.2",
    "datasets >=2.14.3",
    "accelerate >=0.27.2",
    "peft >=0.10.0",
    "trl >=0.8.1",
    "gradio >=4.0.0",
    "scipy",
    "einops",
    "sentencepiece",
    "protobuf",
    "uvicorn",
    "pydantic",
    "fastapi",
    "sse-starlette",
    "matplotlib-base >=3.7.0",
    "fire",
    "galore-torch",
    "tiktoken"
   ]
  },
  "source": {
   "sha256": "6e4ef03e30bcc4f02744715326fee68d7c7a184bb767699234fb6e548e54fd47",
   "url": "https://github.com/hiyouga/LLaMA-Factory/archive/v0.8.2.tar.gz"
  },
  "test": {
   "commands": [
    "pip check"
   ],
   "imports": [
    "llamafactory"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python",
    "setuptools"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "accelerate",
    "datasets",
    "einops",
    "fastapi",
    "fire",
    "galore-torch",
    "gradio",
    "matplotlib-base",
    "packaging",
    "peft",
    "protobuf",
    "pydantic",
    "python",
    "pytorch",
    "pyyaml",
    "scipy",
    "sentencepiece",
    "sse-starlette",
    "tiktoken",
    "transformers",
    "trl",
    "uvicorn"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "meta_yaml": {
  "about": {
   "home": "https://github.com/hiyouga/LLaMA-Factory",
   "license": "Apache-2.0",
   "license_file": "LICENSE",
   "summary": "Easy-to-use LLM fine-tuning framework"
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "PYTHON -m pip install . -vv --no-deps --no-build-isolation"
  },
  "extra": {
   "recipe-maintainers": [
    "jan-janssen"
   ]
  },
  "package": {
   "name": "llmtuner",
   "version": "0.8.2"
  },
  "requirements": {
   "host": [
    "python >=3.8",
    "setuptools >=61.0",
    "pip"
   ],
   "run": [
    "pyyaml",
    "packaging",
    "python >=3.8",
    "pytorch >=1.13.1",
    "transformers >=4.37.2",
    "datasets >=2.14.3",
    "accelerate >=0.27.2",
    "peft >=0.10.0",
    "trl >=0.8.1",
    "gradio >=4.0.0",
    "scipy",
    "einops",
    "sentencepiece",
    "protobuf",
    "uvicorn",
    "pydantic",
    "fastapi",
    "sse-starlette",
    "matplotlib-base >=3.7.0",
    "fire",
    "galore-torch",
    "tiktoken"
   ]
  },
  "source": {
   "sha256": "6e4ef03e30bcc4f02744715326fee68d7c7a184bb767699234fb6e548e54fd47",
   "url": "https://github.com/hiyouga/LLaMA-Factory/archive/v0.8.2.tar.gz"
  },
  "test": {
   "commands": [
    "pip check"
   ],
   "imports": [
    "llamafactory"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "name": "llmtuner",
 "outputs_names": {
  "__set__": true,
  "elements": [
   "llmtuner"
  ]
 },
 "parsing_error": false,
 "platforms": [
  "linux_64"
 ],
 "pr_info": {
  "__lazy_json__": "pr_info/llmtuner.json"
 },
 "raw_meta_yaml": "{% set name = \"llmtuner\" %}\n{% set version = \"0.8.2\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://github.com/hiyouga/LLaMA-Factory/archive/v{{ version }}.tar.gz\n  sha256: 6e4ef03e30bcc4f02744715326fee68d7c7a184bb767699234fb6e548e54fd47\n\nbuild:\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation\n  number: 0\n\nrequirements:\n  host:\n    - python >=3.8\n    - setuptools >=61.0\n    - pip\n  run:\n    - pyyaml\n    - packaging\n    - python >=3.8\n    - pytorch >=1.13.1\n    - transformers >=4.37.2\n    - datasets >=2.14.3\n    - accelerate >=0.27.2\n    - peft >=0.10.0\n    - trl >=0.8.1\n    - gradio >=4.0.0\n    - scipy\n    - einops\n    - sentencepiece\n    - protobuf\n    - uvicorn\n    - pydantic\n    - fastapi\n    - sse-starlette\n    - matplotlib-base >=3.7.0\n    - fire\n    - galore-torch\n    - tiktoken\n\ntest:\n  imports:\n    - llamafactory\n  commands:\n    - pip check\n  requires:\n    - pip\n\nabout:\n  home: https://github.com/hiyouga/LLaMA-Factory\n  summary: Easy-to-use LLM fine-tuning framework\n  license: Apache-2.0\n  license_file: LICENSE\n\nextra:\n  recipe-maintainers:\n    - jan-janssen\n",
 "req": {
  "__set__": true,
  "elements": [
   "accelerate",
   "datasets",
   "einops",
   "fastapi",
   "fire",
   "galore-torch",
   "gradio",
   "matplotlib-base",
   "packaging",
   "peft",
   "pip",
   "protobuf",
   "pydantic",
   "python",
   "pytorch",
   "pyyaml",
   "scipy",
   "sentencepiece",
   "setuptools",
   "sse-starlette",
   "tiktoken",
   "transformers",
   "trl",
   "uvicorn"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python",
    "setuptools"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "accelerate",
    "datasets",
    "einops",
    "fastapi",
    "fire",
    "galore-torch",
    "gradio",
    "matplotlib-base",
    "packaging",
    "peft",
    "protobuf",
    "pydantic",
    "python",
    "pytorch",
    "pyyaml",
    "scipy",
    "sentencepiece",
    "sse-starlette",
    "tiktoken",
    "transformers",
    "trl",
    "uvicorn"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python >=3.8",
    "setuptools >=61.0"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "accelerate >=0.27.2",
    "datasets >=2.14.3",
    "einops",
    "fastapi",
    "fire",
    "galore-torch",
    "gradio >=4.0.0",
    "matplotlib-base >=3.7.0",
    "packaging",
    "peft >=0.10.0",
    "protobuf",
    "pydantic",
    "python >=3.8",
    "pytorch >=1.13.1",
    "pyyaml",
    "scipy",
    "sentencepiece",
    "sse-starlette",
    "tiktoken",
    "transformers >=4.37.2",
    "trl >=0.8.1",
    "uvicorn"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "url": "https://github.com/hiyouga/LLaMA-Factory/archive/v0.8.2.tar.gz",
 "version": "0.8.2",
 "version_pr_info": {
  "__lazy_json__": "version_pr_info/llmtuner.json"
 }
}